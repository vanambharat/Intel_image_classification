{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intel_Image_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoT6ODFoVz8K"
      },
      "source": [
        "# Intel Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhWtJlWV7m6"
      },
      "source": [
        "In this project, I worked on Intel images i.e. images of buildings, forest, street, etc. I  built a convolutional neural network and train it on this images. This is a multi class classification problem and I used Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUw6Lm1ONoq4"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j53m7mK_1yxb"
      },
      "source": [
        "First I mounted my google drive on colab so that I can use the dataset directly from my drive. For this I first upload the data on my drive and then mounted the drive on colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhaKLEACN-NH"
      },
      "source": [
        "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCkhamd_11rC"
      },
      "source": [
        "After mounting our drive we will locate the folder where our data is stored to use it in our colab notebook. Here we will see all the folders I have in my drive and 'Intel Image Dataset' contains the images that we will work on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgD05QX3N-Ox"
      },
      "source": [
        "# Important imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import cv2\n",
        "import random\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import  LabelBinarizer\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, LeakyReLU\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cSEn4h01__w"
      },
      "source": [
        "I started by importing some required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxImdzDnRWQn"
      },
      "source": [
        "# Listing directory\n",
        "!ls \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBh7ypts2DQ0"
      },
      "source": [
        "I checked for folders of class images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzPXISMUN-Sq"
      },
      "source": [
        "# Plotting 25 images to check dataset\n",
        "plt.figure(figsize=(11,11))\n",
        "path = \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset/mountain\"\n",
        "for i in range(1,26):\n",
        "    plt.subplot(5,5,i)\n",
        "    plt.tight_layout()\n",
        "    rand_img = imread(path +'/'+ random.choice(sorted(listdir(path))))\n",
        "    plt.imshow(rand_img)\n",
        "    plt.title('mountain')\n",
        "    plt.xlabel(rand_img.shape[1], fontsize = 10)\n",
        "    plt.ylabel(rand_img.shape[0], fontsize = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puab6nG92K2k"
      },
      "source": [
        "Let's visualize some of the mountain images that I will be working on. Also I observed x and y dimensions of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdyiHpahReF8"
      },
      "source": [
        "# Setting root directory path and creating empty list\n",
        "dir = \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset\"\n",
        "root_dir = listdir(dir)\n",
        "image_list, label_list = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgcSBgpU2T_z"
      },
      "source": [
        "Setting the root directory for the dataset and storing all the folders name of the dataset. I also created 2 empty list for image and lables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLoajsN-RmQF"
      },
      "source": [
        "# Reading and converting image to numpy array\n",
        "for directory in root_dir:\n",
        "  for files in listdir(f\"{dir}/{directory}\"):\n",
        "    image_path = f\"{dir}/{directory}/{files}\"\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((150,150)) # All images does not have same dimension\n",
        "    image = img_to_array(image)\n",
        "    image_list.append(image)\n",
        "    label_list.append(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGe05NlN2X5B"
      },
      "source": [
        "Next I needed to resize images as some of the images don't have same dimensions. So, I read and resize all the images. Then I converted it into array and appending the list created above with the image and its label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX89xh5BRmlG"
      },
      "source": [
        "# Visualize the number of classes count\n",
        "label_counts = pd.DataFrame(label_list).value_counts()\n",
        "label_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSQA5G8L57wY"
      },
      "source": [
        "Checking for images per class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Pgx3XMRoaP"
      },
      "source": [
        "# Checking count of classes\n",
        "num_classes = len(label_counts)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhRcDCf5-eN"
      },
      "source": [
        "Storing the number of classes which will be used further in model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJp5JRT0Rp7e"
      },
      "source": [
        "# Checking x data shape\n",
        "np.array(image_list).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5Lol8U6Shy"
      },
      "source": [
        "Check the shape of the x data for input layer of model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_aIkXkyRt2d"
      },
      "source": [
        "# Checking y data shape\n",
        "label_list = np.array(label_list)\n",
        "label_list.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qs2_pO16fxA"
      },
      "source": [
        "Checking the number of labels in y data which should be equal to total number of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhTfmSvoRt5l"
      },
      "source": [
        "# Splitting dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp2n-QTR65g1"
      },
      "source": [
        "Now I splited my dataset into testing and training using train_test_split() from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5GWA6z5Rxtm"
      },
      "source": [
        "# Normalize and reshape data\n",
        "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
        "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
        "x_train = x_train.reshape( -1, 150,150,3)\n",
        "x_test = x_test.reshape( -1, 150,150,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxub0cgO7EzH"
      },
      "source": [
        "Next I normalized the images by dividing them with 255 and I also reshaped x_train and x_test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ8NTYmfRzOS"
      },
      "source": [
        "# Binarizing labels\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "print(lb.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuVKsoqF7XK_"
      },
      "source": [
        "Here I used label binarizer to one hot encode my y data. I also printed the sequence of the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JshCofPfR01i"
      },
      "source": [
        "# Splitting the training data set into training and validation data sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34xAE2Nz7iEo"
      },
      "source": [
        "Now we will split the training data to validation and training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN8XnsG9R2oA"
      },
      "source": [
        "# Creating model architecture\n",
        "model = Sequential([\n",
        "        Conv2D(16, kernel_size = (3,3), input_shape = (150,150,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "          \n",
        "        Conv2D(32, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "        \n",
        "        Conv2D(64, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Conv2D(128, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "\n",
        "        Flatten(),\n",
        "    \n",
        "        Dense(64),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Dense(32),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "    \n",
        "        Dense(16),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(1),\n",
        "    \n",
        "        Dense(6, activation = 'softmax')    \n",
        "        ])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyAv8tUF72vw"
      },
      "source": [
        "Now I created a network architecture for the model. I have used different types of layers according to their features namely BatchNormalization (Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch), LeakyRelu (The Leaky ReLU modifies the function to allow small negative values when the input is less than zero), Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel). In the last layer I used softmax as the activation function because it is a multi class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvVgEiXR4sy"
      },
      "source": [
        "# Compiling model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Uhzshq8bci"
      },
      "source": [
        "For compiling the model I need to pass 3 parameters namely loss, optimizer and metrics. Here I will use loss as categorical_crossentropy, optimizer as Adam and metrics as accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1I9HtY2R7R-"
      },
      "source": [
        "# Training the model\n",
        "epochs = 70\n",
        "batch_size = 128\n",
        "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJ2AIYK8y2t"
      },
      "source": [
        "Fitting the model with the data and finding out the accuracy at each epoch to see how our model is learning. Now I trained my model on 70 epochs and a batch size of 128. I can try using more number of epochs to increase accuracy. During each epochs I saw how the model is performing by viewing the training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZBnscUwSDi5"
      },
      "source": [
        "# Saving model\n",
        "model.save(\"/content/drive/My Drive/intel_image.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baX1EiUc9LnF"
      },
      "source": [
        "We will save the model using model.save() to use it later for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBEz7EGGSFro"
      },
      "source": [
        "#Plot the training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['accuracy'], color='r')\n",
        "plt.plot(history.history['val_accuracy'], color='b')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFQaRBDO9VEJ"
      },
      "source": [
        "Next we will plot the accuracy of the model for the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auY2U-1USIk2"
      },
      "source": [
        "#Plot the loss history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['loss'], color='r')\n",
        "plt.plot(history.history['val_loss'], color='b')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdJ074B9bqu"
      },
      "source": [
        "Next we will plot the loss of the model for the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRI9dX9eSI0j"
      },
      "source": [
        "# Calculating test accuracy \n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiGjHNML9f_B"
      },
      "source": [
        "Evaluating the model to know the accuracy of the model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_931pQbeSKyO"
      },
      "source": [
        "# Storing model predictions\n",
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4MrMF8Q9meA"
      },
      "source": [
        "Generating predictions for test data and storing them into y_pred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNByJOaYSMNh"
      },
      "source": [
        "# Plotting image to compare\n",
        "img = array_to_img(x_test[1])\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdmALWF9rYp"
      },
      "source": [
        "Visualizing an image to be predicted in further steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqAQkqooSQAo"
      },
      "source": [
        "# Finding max value from predition list and comaparing original value vs predicted\n",
        "labels = lb.classes_\n",
        "print(labels)\n",
        "print(\"Originally : \",labels[np.argmax(y_test[1])])\n",
        "print(\"Predicted : \",labels[np.argmax(y_pred[1])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EreThly90vE"
      },
      "source": [
        "Now, I will create list of labels using object of label binarizer. I will print that list and finally I will print out the prediction and the original label of the image we visualized above using argmax()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ndQ_o5V_r2"
      },
      "source": [
        "## Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FXJgeq7WDBg"
      },
      "source": [
        "In this project I saw how I can create a CNN using different layers. Normalizing is an important step when working with any type of dataset. I will use this model to predict the class of the image supplied to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni7-suUgWF-Z"
      },
      "source": [
        "## Scope:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIaqIL-3-sKo"
      },
      "source": [
        "This project has a vast scope, it can be used to classify satellite images, drone images, google images into different classes like sea, mountain, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3gqSpVKSSx3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}